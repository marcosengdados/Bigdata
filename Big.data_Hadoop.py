# -*- coding: utf-8 -*-
"""Bigdata.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ro-F8S_uO2AMR85oFAc2_ZBv6OAfYNJx

## üåç BigData Clean & Transform ‚Äì UK Macroeconomy

BigData Clean & Transform √© um projeto desenvolvido com Apache Spark em Python (PySpark), voltado para o tratamento, limpeza e transforma√ß√£o de dados econ√¥micos macroestruturais do Reino Unido. Ele exemplifica boas pr√°ticas em engenharia de dados para projetos que exigem processamento distribu√≠do, manipula√ß√£o de datasets grandes e automa√ß√£o de rotinas anal√≠ticas em ambiente cloud.

| üí° Recurso                                   | üß© Descri√ß√£o                                                                                                         |
| -------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |
| **Setup autom√°tico do Spark 3.0**            | Inclui configura√ß√£o do ambiente Spark/Hadoop e depend√™ncias em notebooks Colab para r√°pida execu√ß√£o em nuvem.        |
| **Ingest√£o de dados econ√¥micos**             | Carregamento direto de dados CSV macroecon√¥micos (popula√ß√£o e taxa de desemprego) via `wget` remoto.                 |
| **Data Wrangling com PySpark**               | Processos robustos de renomea√ß√£o, filtragem sem√¢ntica, tratamento de valores nulos e deduplica√ß√£o.                   |
| **Filtragem sem√¢ntica com `left_anti` join** | Remove registros descritivos n√£o anal√≠ticos, como linhas com ‚ÄúUnits‚Äù, garantindo dados puros para an√°lise.           |
| **Engenharia de vari√°veis temporais**        | Cria√ß√£o de coluna `century` a partir do ano, permitindo agrupamentos hist√≥ricos de maneira eficiente.                |
| **Paraleliza√ß√£o e grava√ß√£o particionada**    | Dados s√£o reparticionados por s√©culo e exportados em CSV, prontos para an√°lises posteriores ou ingest√£o em sistemas. |
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz
# 
# !tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !apt-get remove openjdk*
# !apt-get update --fix-missing
# !apt-get install openjdk-8-jdk-headless -qq > /dev/null

!pip install -q pyspark==3.0.0

!pip install py4j

import os

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.0-bin-hadoop2.7"

!pip install -q findspark==1.4.2

import findspark

findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[*]").appName("pyspark-notebook").getOrCreate()

# Comando para baixar o arquivo CSV
!wget -q "https://raw.githubusercontent.com/cluster-apps-on-docker/spark-standalone-cluster-on-docker/master/build/workspace/data/uk-macroeconomic-data.csv" -O "uk-macroeconomic-data.csv"

data = spark.read.csv(path="uk-macroeconomic-data.csv", sep=",", header=True)

data.show()

data.printSchema()

"""- Data Wrangling"""

data.count()

data.columns

len(data.columns)

data = data.select([ "Description", "Population (GB+NI)", "Unemployment rate" ])

data = data.withColumnRenamed("Description", 'year').withColumnRenamed("Population (GB+NI)", "population").withColumnRenamed("Unemployment rate", "unemployment_rate")

data.show(n=10)

data_description = data.filter(data['year'] == 'Units')

data_description.show(n=10)

(data.count(), len(data.columns))

(data_description.count(), len(data_description.columns))

from pyspark.sql.functions import broadcast

data = data.join( other=broadcast(data_description) , on=['year'], how='left_anti' )

data.show(n=10)

data = data.dropna()

data.show(n=10)

data = data.withColumn( 'century', 1 + (data['year']/100).cast('int') )

data.select(['century', 'year']). groupBy('century').agg({'year': 'count'}).show()

timing = data.select(['century', 'year']).groupBy('century').agg({'year': 'count'}).collect()

timing

timing[0].asDict()

data.repartition('century').write.csv( path="uk-macroeconomic-data-clean", sep=",", header=True, mode="overwrite" )

"""## Diferenciais e Valor Agregado

‚úÖ Escalabilidade: Pronto para processar milh√µes de linhas com o poder do Spark.

‚úÖ Integra√ß√£o com pipelines automatizados: Ideal para ser acoplado a workflows de ETL em sistemas cloud-native.

‚úÖ Foco em dados limpos e confi√°veis: Aplica√ß√£o clara de boas pr√°ticas em data cleaning e pr√©-processamento.

‚úÖ Ambiente port√°til e replic√°vel: Projetado para execu√ß√£o em ambientes como Google Colab ou clusters Spark.
"""